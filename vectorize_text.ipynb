{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, RobertaModel, PhobertTokenizer\n",
    "import torch\n",
    "import py_vncorenlp\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r\"\"\n",
    "df = pd.read_csv(filepath, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2id = {class_: id for id, class_ in enumerate(df['Department'].unique())}\n",
    "def Labeling(label, multiclass=True):\n",
    "    \"\"\"Encoding text label\n",
    "\n",
    "    Args:\n",
    "        label (str): The label in the string format\n",
    "        multiclass (bool, optional): True for Multiclass labeling, otherwise Multilabel. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        int or list[int]: transformed label\n",
    "    \"\"\"\n",
    "    # Multiclass\n",
    "    if multiclass: return class2id[label]\n",
    "    \n",
    "    # MultiLabel if multiclass = False\n",
    "    multi_label = [0] * len(class2id)\n",
    "    multi_label[class2id[label]] = 1\n",
    "    \n",
    "    return multi_label\n",
    "\n",
    "df['Label'] = df['Department'].apply(Labeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phobert_model = RobertaModel.from_pretrained(\"vinai/phobert-base\")\n",
    "tokenizer = PhobertTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "device = torch.device(\"cuda\" if torch.accelerator.is_available() else \"cpu\")\n",
    "phobert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfMatrix:\n",
    "    _instance = None\n",
    "\n",
    "    @classmethod\n",
    "    def get_instance(cls, corpus=None):\n",
    "        \"\"\"\n",
    "        Returns the singleton instance of TfidfVectorizer. If the instance doesn't exist,\n",
    "        it is created and optionally fitted on the provided corpus.\n",
    "        \n",
    "        Args:\n",
    "            corpus (iterable, optional): An iterable of text documents used to fit the vectorizer.\n",
    "                                          This is only used when creating the instance for the first time\n",
    "        \"\"\"\n",
    "        if cls._instance is None:\n",
    "            cls._instance = TfidfVectorizer()\n",
    "            if corpus is not None and len(corpus) > 0:\n",
    "                cls._instance.fit(corpus)\n",
    "            else:\n",
    "                pass\n",
    "        return cls._instance\n",
    "    \n",
    "\n",
    "def transform(sentence, pooling='mean', state='static', vncore=False, tfidf_weight=False):\n",
    "    \"\"\"\n",
    "    Chuyển đổi một câu thành vector biểu diễn sử dụng PhoBERT.\n",
    "    \n",
    "    Hàm này thực hiện các bước:\n",
    "      - Tiền xử lý câu: Có thể dùng VnCoreNLP để tách từ nếu cần.\n",
    "      - Token hóa câu: Chuyển câu thành các token dưới dạng tensor.\n",
    "      - Trích xuất embedding: Sử dụng pooling trung bình hoặc lấy token [CLS] dựa vào tham số pooling.\n",
    "      - Tùy chọn: Áp dụng trọng số TF-IDF cho các embedding.\n",
    "      \n",
    "    Tham số:\n",
    "        sentence (str): Câu cần chuyển đổi.\n",
    "        pooling (str, tùy chọn): Chiến lược pooling ('mean' để tính trung bình, 'cls' để dùng token [CLS]).\n",
    "        state (str, tùy chọn): Chế độ embedding khi dùng mean pooling ('static' cho embedding tiền huấn luyện, 'context' cho embedding ngữ cảnh). \n",
    "                                Lưu ý: Bị bỏ qua nếu pooling là 'cls'.\n",
    "        vncore (bool, tùy chọn): Nếu True, áp dụng VnCoreNLP để tách câu (ví dụ: chuyển \"căng thẳng\" thành \"căng_thẳng\").\n",
    "        tfidf_weight (bool, tùy chọn): Nếu True, áp dụng trọng số TF-IDF lên các token embeddings.\n",
    "        \n",
    "    Trả về:\n",
    "        numpy.ndarray: Mảng numpy chứa vector của câu với kích thước (1, hidden_dim). Hidden_dim thường là 768.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Xử lý câu bằng cách VnCoreNLP.\n",
    "    if vncore:\n",
    "        try: \n",
    "            segmentor = py_vncorenlp.VnCoreNLP(\n",
    "                save_dir=r\"M:\\Python\\symptom_disease\\vncorenlp\",\n",
    "                annotators=['wseg'],\n",
    "                max_heap_size=\"-Xmx500m\"\n",
    "            )\n",
    "            sentence = ' '.join(segmentor.word_segment(sentence))\n",
    "        except (OSError, ValueError):\n",
    "            pass\n",
    "    \n",
    "    tokenized = tokenizer(text=sentence, padding='max_length', truncation=True, max_length=256, return_tensors='pt')\n",
    "    # Di chuyển các tensor của token sang GPU\n",
    "    tokenized = {key: value.to(device) for key, value in tokenized.items()}\n",
    "        \n",
    "    if pooling == 'mean':\n",
    "        # Tùy chọn chế độ embedding: nếu 'context' thì dùng context embedding, nếu không thì dùng static embedding \n",
    "        if state == 'context': \n",
    "            # Lấy context embedding từ last_hidden_state\n",
    "            hidden_states = phobert_model(**tokenized).last_hidden_state\n",
    "        else: \n",
    "            # Lấy static embedding từ embedding layer ( first layer)\n",
    "            hidden_states = phobert_model.embeddings.word_embeddings(tokenized[\"input_ids\"])\n",
    "        \n",
    "        # TF-IDF weighting\n",
    "        if tfidf_weight:\n",
    "            # TF-IDF matrix \n",
    "            tfidf = TfidfMatrix.get_instance(corpus=df['Symptom'])\n",
    "            # Chuyển câu sang TF-IDF vector \n",
    "            tfidf_vector = tfidf.transform([sentence])\n",
    "            \n",
    "            # Chuyển các token id thành từ để so sánh với vocabulary của TF-IDF\n",
    "            word_tokens = tokenizer.convert_ids_to_tokens(tokenized['input_ids'][0])\n",
    "            \n",
    "            token_weights = []  \n",
    "            for token in word_tokens:\n",
    "                # Tìm weight của token trong vocabulary, nếu không có mặc định là 0.0\n",
    "                idx = tfidf.vocabulary_.get(token, None)\n",
    "                if idx is not None:\n",
    "                    token_weights.append(tfidf_vector[0, idx])\n",
    "                else:\n",
    "                    token_weights.append(float(0.0))\n",
    "                    \n",
    "            # Chuyển danh sách trọng số thành tensor và thay đổi kích thước để phù hợp với hidden_states\n",
    "            token_weights = torch.tensor(token_weights).unsqueeze(0).unsqueeze(-1).to(device)  # Kích thước: (1, seq_len, 1)\n",
    "            # Nhân các hidden_states với trọng số TF-IDF của từng token\n",
    "            hidden_states *= token_weights\n",
    "            \n",
    "        # Reshape attention mask để loại padded tokens\n",
    "        mask_expanded = tokenized['attention_mask'].unsqueeze(-1).expand(hidden_states.size())\n",
    "        # sum embedding của các non-padded token \n",
    "        sum_embeddings = torch.sum(hidden_states * mask_expanded, dim=1)\n",
    "        # số lượng non-padded token \n",
    "        valid_tokens = mask_expanded.sum(dim=1)\n",
    "        # Tránh chia cho 0 bằng cách đảm bảo số token tối thiểu là 1\n",
    "        valid_tokens = torch.clamp(valid_tokens, min=1)\n",
    "        # Tính sentence vector bằng cách lấy mean của token vectors\n",
    "        mean_embedding = sum_embeddings / valid_tokens\n",
    "        sentence_embedding = mean_embedding.detach().cpu().numpy()\n",
    "        \n",
    "        return sentence_embedding\n",
    "    \n",
    "    elif pooling == 'cls':\n",
    "        # Với phương pháp CLS, lấy trực tiếp CLS token từ last_hidden_state.\n",
    "        hidden_states = phobert_model(**tokenized).last_hidden_state\n",
    "        cls_embedding = hidden_states[:, 0, :]\n",
    "        return cls_embedding.detach().cpu().numpy()\n",
    "    \n",
    "    # Don't mind this\n",
    "    return phobert_model.embeddings.word_embeddings(tokenized[\"input_ids\"]).mean(dim=1).detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = df.loc[1].Symptom \n",
    "transform(sentence=sentence, pooling='mean', state='static', vncore=True, tfidf_weight=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.DataFrame({f\"v_{i}\":[] for i in range(1, 769)})\n",
    "\n",
    "for value in df['Symptom']:\n",
    "    static_vector = transform(sentence=value, pooling='mean', state='static', vncore=True, tfidf_weight=True)\n",
    "    merged_df.loc[len(merged_df)] = np.hstack(static_vector.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Department'] = df['Department']\n",
    "merged_df['Category'] = df['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(r\"results\\weighted_static.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ModelService",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
